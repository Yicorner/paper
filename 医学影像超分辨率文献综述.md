# 医学影像超分辨率重建方法综述

## 1. 研究方向概述

医学影像超分辨率（Medical Image Super-Resolution, MISR）是医学图像处理领域的重要研究方向，旨在从低分辨率（Low-Resolution, LR）医学影像中重建出高分辨率（High-Resolution, HR）影像，以提升影像质量、增强细节信息，从而辅助临床诊断和治疗决策。随着深度学习技术的快速发展，医学影像超分辨率方法经历了从传统插值方法到基于深度学习的生成式模型的演进过程。

当前医学影像超分辨率研究主要分为两大分支：非生成式模型超分辨率和生成式模型超分辨率。非生成式模型超分辨率方法包括传统插值方法（如双三次插值、B-spline插值）、基于卷积神经网络（CNN）的方法、基于Transformer的方法以及各种自定义网络结构。这类方法通常通过端到端的学习方式建立LR到HR的映射关系，在生成速度和稳定性方面表现良好，但在细节丰富度上仍存在一定局限性。生成式模型超分辨率方法则包括基于扩散模型（Diffusion Model）的方法、基于生成对抗网络（GAN）的方法以及基于自回归模型的方法。这类方法能够更好地建模复杂的数据分布，在生成高质量细节方面具有显著优势，但往往面临计算成本高、推理速度慢等挑战。

在非生成式模型方面，传统插值方法如双三次插值、压缩感知和稀疏表示等属于非学习方法，无法有效恢复图像的高频细节，原因在于其难以建立HR与LR图像之间复杂的非线性映射。基于CNN的方法通过深度卷积网络学习特征表示，在医学影像超分辨率任务中取得了显著进展。近年来，Transformer架构在视觉任务中展现出强大的全局建模能力，许多研究将CNN与Transformer架构结合，以提升医学图像超分辨率的效果。在生成式模型方面，扩散模型通过随机迭代去噪过程从高斯噪声中合成数据样本，在医学图像超分辨率中展现出巨大潜力。GAN通过生成器与判别器的对抗训练，能够生成具有丰富细节的超分辨率图像。自回归模型则通过逐步预测图像token或尺度来实现超分辨率重建，在保持高可扩展性的同时显著提升了图像的真实性。

## 2. 国内外研究现状

### 2.1 非生成式模型超分辨率

#### 2.1.1 传统插值方法

传统的图像超分辨率方法主要基于插值技术，这些方法属于非学习方法，无法有效恢复图像的高频细节（如纹理），原因在于其难以建立HR与LR图像之间复杂的非线性映射。常用的插值方法包括双三次插值（bicubic）和B-spline插值等。然而，这些方法会在超分辨图像中引入边缘模糊和块状伪影，导致临床医生无法做出准确诊断。

传统SR算法通常利用变换域中的冗余信息进行MRI超分辨率重建，例如迭代去模糊算法、低秩方法和字典学习等。压缩感知方法通过利用信号的稀疏性，在欠采样条件下重建高分辨率图像。稀疏表示方法则通过学习字典来稀疏表示图像块，从而实现超分辨率重建。然而，当放大倍数（upsampling factor, UF）较大时，这些方法生成的SR图像质量往往不令人满意，难以满足临床诊断的需求。

#### 2.1.2 基于CNN的方法

在深度学习自然图像超分辨率方法以及CT超分辨率方法的研究推动下，一些优秀的MRI超分辨率重建方法陆续出现。基于CNN的方法通过深度卷积网络学习从LR到HR的映射关系，在医学影像超分辨率任务中取得了显著进展。

许多研究探讨了将CNN与Transformer架构结合，以提升医学图像超分辨率的效果。Feng等人[11]在网络的不同阶段进行多对比度特征融合，以捕获融合特征之间的依赖关系，从而增强其表征能力。Zhang等人[58]引入了带残差缩放的挤压与激励（squeeze-and-excitation）注意力机制，不仅稳定了训练过程，还实现了更精确的细纹理和解剖结构细节重建。该方法本质上是一种轻量级模型，更适合在实际临床应用中进行部署。

在单对比度MRI超分辨率方面，Qiu等人使用卷积神经网络（CNN）进行膝关节MRI的SR重建；Lyu等人使用集成学习进行MRI SR重建；Li等人在GAN中引入注意力机制和循环损失用于盆腔图像的SR重建。Zhang等人提出了挤压与激励推理注意力网络来实现MR图像SR重建，实验结果验证了该方法的有效性。然而，上述算法均仅使用单一对比度的MR图像来进行重建。

注意力机制赋予神经网络对输入特征进行自适应资源分配的能力，有助于充分挖掘网络的表示能力，从而提升模型性能。在图像超分等低层视觉任务中，也已有一些研究尝试将注意力机制引入神经网络结构。但较少有工作专门研究在MRI单幅图像超分辨率任务中使用注意力机制的效果，而MRI图像具有其独特特性，例如重复模式、结构相对简单以及背景信息较少。如果网络能够根据MRI图像中的有效区域自适应地分配计算资源，那么在保持模型参数适中的前提下，有望进一步提升重建性能。

尽管这些基于CNN的方法在生成速度和稳定性方面表现良好，但在细节丰富度上仍弱于生成式模型，难以刻画复杂纹理与细微解剖差异。

#### 2.1.3 基于Transformer的方法

与卷积神经网络（CNN）不同，Transformer使用自注意力机制来获取上下文之间的全局信息，并且在处理视觉问题方面取得了更好的效果。此外，一些研究也证明了Transformer在MRI重建中的有效性。

Feng等人使用任务Transformer网络将MRI重建与超分辨率重建结合起来，并提出采用多模态Transformer用于多对比度MRI重建。然而，传统的Transformer是以图像块（patch）为输入进行处理的，这会导致图像边缘像素无法学习到块外邻域像素的信息。

Swin Transformer可以用来解决上述问题，它结合了CNN和传统Transformer的优势。该方法通过引入窗口平移机制（shifted window scheme）来建立长距离依赖关系，从而解决了patch处理方式下边缘像素信息受限的问题。Liang等人采用混合方法，先使用CNN层提取浅层特征，再通过Swin Transformer模块进行层级建模，以实现更有效的全局特征聚合和细粒度细节重建。

Li等人利用基于Transformer的模块来促进多尺度上下文匹配，显著提升了特征融合质量。在多对比度MRI超分辨率方面，多对比度MRI SR的关键问题是如何获取参考图像，以更好地指导目标图像的SR重建。Lyu等人表明，在高层特征空间融合多对比度信息的效果优于在低层像素空间进行组合。因此，从深层特征空间进行多对比度特征匹配与聚合，可以充分利用参考图像中的信息。

Feng等人提出了一种多阶段特征融合机制用于多对比度SR，即上一阶段的参考特征与目标特征融合生成集成特征，用于指导下一阶段目标特征的学习。受此启发，在上采样过程中考虑融合不同尺度参考图像的特征。具体而言，在深层特征空间进行多尺度上下文匹配与聚合，并利用多尺度匹配得到的参考特征来指导目标HR特征的恢复。

#### 2.1.4 自定义网络结构

除了标准的CNN和Transformer架构，许多研究还提出了针对医学影像超分辨率任务的自定义网络结构。Lei等人观察到多对比度MR图像在结构信息（如解剖结构与边缘）上具有一致性，但在对比度上存在差异。基于这一观察，他们提出了一种分解式变分网络，用于解耦并重建共享与模态特定的成分。

Georgescu等人设计了多头空间注意力机制，其中每个注意力头具有不同的感受野，以捕获不同尺度的空间信息。这种设计使得网络能够同时关注不同尺度的特征，从而更好地重建多尺度细节。

尽管这些自定义网络结构在特定任务上取得了良好效果，但在细节丰富度上仍弱于生成式模型，难以刻画复杂纹理与细微解剖差异。

### 2.2 生成式模型超分辨率

#### 2.2.1 基于扩散模型的方法

扩散模型（DMs）作为一类概率生成模型，通过随机迭代去噪过程从高斯噪声中合成数据样本。在医学图像超分辨率中，尤其是在MRI中，DMs展现出巨大潜力。

Li等人提出了首个基于扩散模型的单图像超分辨率框架，并取得了有前景的结果。Mao等人构建了一个用于多对比度MRI的条件扩散模型，实现了结构化的图像超分辨率。Chung等人通过在反向扩散过程中初始化单次前向推理，减少了所需的采样步骤，从而降低了计算成本。此外，Chung等人提出了用于加速MRI重建的基于score的扩散模型。

Gao等人提出了一种隐式扩散模型，将潜在扩散与条件强化机制相结合，以实现高保真与连续性的图像超分辨率。该方法通过将扩散过程应用于潜在空间而非像素空间，显著降低了计算复杂度，同时保持了生成质量。

最近，Li等人引入了用于加速去噪步骤的扩散先验模型，并将其整合到一个基于Transformer的多对比度超分辨率框架中，从而实现高效的MRI图像生成。该方法通过引入扩散先验，在保持生成质量的同时显著提升了推理速度。

扩散模型能够更稳定地建模复杂分布，在医学影像生成中表现优异，但其多步迭代推理导致计算代价高、速度慢，限制了临床应用。因此，如何加速扩散模型的推理过程，在保持生成质量的同时提升效率，是当前研究的重要方向。

#### 2.2.2 基于GAN的方法

生成对抗网络（GAN）通过生成器与判别器的对抗训练，能够生成具有丰富细节的超分辨率图像。在医学影像超分辨率任务中，GAN方法展现出了强大的生成能力。

GAN-based超分辨率方法的核心思想是通过对抗训练，使生成器能够生成与真实高分辨率图像难以区分的结果。生成器负责从低分辨率图像生成高分辨率图像，而判别器则负责区分生成图像与真实高分辨率图像。这种对抗机制促使生成器不断改进，生成更加逼真的细节。

在医学影像超分辨率中，GAN方法能够有效恢复高频细节，生成具有丰富纹理的超分辨率图像。然而，GAN训练过程存在不稳定性问题，容易出现模式崩塌，且生成的图像可能存在伪影。为了缓解这些问题，研究者们提出了各种改进方法，如引入感知损失、特征匹配损失等，以提升生成质量和训练稳定性。

尽管GAN方法在生成细节方面具有优势，但其训练不稳定性和潜在的伪影问题仍然限制了其在医学影像超分辨率中的应用。因此，如何提升GAN训练的稳定性和生成图像的真实性，是当前研究的重要挑战。

（注：本部分内容参考了GAN based super-resolution.pdf，具体参考文献标注见该PDF文件中的相关引用）

#### 2.2.3 基于自回归模型的方法

视觉自回归模型通常采用类似VQ-VAE的框架，将图像转化为离散序列，从而实现下一token预测以用于图像生成。然而，该方法破坏了二维图像的固有空间结构，且通常生成较慢。为解决这一效率问题，Chang等人提出了迭代的掩码token预测模型，大幅加速了图像生成过程。但这些技术仍难以完全保持图像的空间连续性与结构完整性。

为突破这一限制，Fan等人提出了从下一token预测到下一尺度预测的生成范式转变。该层级式生成策略在保持高可扩展性的同时，显著提升了图像的真实性。在此基础上，基于视觉自回归的模型（VAR-based models）在多种生成任务中表现出色，包括文本生成图像、图像生成图像、图像修复以及图像超分辨率等。

自回归模型可以直接对像素进行回归，但主要是配合VAE来使用。根据是否依赖VAE，可以将自回归模型分为三类：

第一类是纯自回归模型（直接对像素），不依赖VAE，代表模型包括PixelRNN、PixelCNN、Gated PixelCNN、Image GPT (iGPT)等。这类方法直接对像素值进行自回归建模，计算复杂度较高。

第二类是基于VQ-VAE（离散表示）的自回归模型，依赖VAE，代表模型包括VQ-VAE＋Transformer、MaskGIT，以及最近的VAR模型。这类方法首先通过VQ-VAE将图像编码为离散token序列，然后使用Transformer等自回归模型对token序列进行建模。这种方法在保持生成质量的同时，显著降低了计算复杂度。

第三类是基于VAE latent（连续）的自回归模型，可依赖VAE，包括一些VAE + autoregressive hybrid研究。其中，Autoregressive Image Generation without Vector Quantization（mar.pdf）直接对连续token建模，避免了离散量化带来的信息损失。

在医学影像超分辨率任务中，自回归模型通过逐步预测图像token或尺度来实现超分辨率重建。Guo等人提出了LAR-SR，一种局部自回归模型用于图像超分辨率。该方法通过局部自回归建模，在保持生成质量的同时提升了推理效率。

自回归模型在保持高可扩展性的同时，显著提升了图像的真实性。然而，传统的自回归模型通常依赖于离散化的量化token，这限制了模型的表示能力与生成质量。为突破这一限制，一些研究在自回归模型中引入了连续空间建模，使得模型能够更充分地保留图像的细节与结构信息。

## 3. 研究现状总结与分析

### 3.1 论文研究领域存在的问题

通过对医学影像超分辨率研究现状的深入分析，我们发现该领域仍存在以下主要问题：

首先，传统插值方法和基于稀疏表示的方法无法有效建立低分辨率与高分辨率影像间的复杂非线性映射关系，难以恢复真实的高频细节。这些方法在处理大放大倍数时，往往会产生边缘模糊和块状伪影，无法满足临床诊断的精度要求。

其次，虽然基于CNN和Transformer的非生成式模型在生成速度和稳定性方面表现良好，但在细节丰富度上仍存在局限性。这些方法容易出现过度平滑、纹理丢失等问题，难以刻画复杂纹理与细微解剖差异。特别是在处理严重退化的低分辨率图像时，这些方法往往无法恢复已经丢失的细节信息。

第三，生成式模型虽然在生成高质量细节方面具有显著优势，但各自存在明显的局限性。扩散模型虽然能够稳定地建模复杂分布，但其多步迭代推理导致计算代价高、速度慢，严重限制了临床应用。GAN方法虽然能够生成丰富的细节，但训练过程存在不稳定性问题，容易出现模式崩塌，且生成的图像可能存在伪影。自回归模型虽然能够保持高可扩展性并提升图像真实性，但传统的自回归模型通常依赖于离散化的量化token，这限制了模型的表示能力与生成质量。

第四，现有方法在处理多对比度医学影像时，如何有效融合不同模态的信息仍是一个挑战。虽然已有研究提出了多阶段特征融合机制和多尺度上下文匹配方法，但在如何充分利用不同对比度图像之间的互补信息方面，仍有改进空间。

第五，现有方法在计算效率与生成质量的平衡方面存在不足。非生成式模型虽然速度快，但质量有限；生成式模型虽然质量高，但计算成本大。如何在保持高生成质量的同时提升计算效率，是当前研究面临的重要挑战。

第六，现有方法在医学影像的特定需求方面考虑不足。医学影像具有其独特特性，例如重复模式、结构相对简单以及背景信息较少，但现有方法往往直接借鉴自然图像超分辨率的方法，没有充分考虑医学影像的特殊性。

### 3.2 论文研究领域的发展趋势

基于对现有问题的分析和对研究现状的梳理，我们认为医学影像超分辨率领域的发展趋势主要体现在以下几个方面：

第一，生成式模型与非生成式模型的融合将成为重要趋势。通过将生成式模型的强大生成能力与非生成式模型的高效推理相结合，有望在保持高生成质量的同时提升计算效率。例如，可以在自回归模型中引入轻量化的扩散正则，在几乎不增加计算开销的前提下，有效提升生成结果的真实性与稳定性。

第二，连续空间建模将逐步替代离散量化。传统的自回归模型依赖于离散化的量化token，限制了模型的表示能力。通过在连续空间中进行建模与优化，模型能够更充分地保留图像的细节与结构信息，从而显著提升生成质量与视觉真实感。非量化的视觉token建模将成为未来研究的重要方向。

第三，多模态融合与跨模态学习将得到更多关注。医学影像往往包含多种对比度或模态的信息，如何有效融合这些信息以提升超分辨率重建质量，将是未来研究的重要方向。通过跨模态学习，可以利用不同模态之间的互补信息，进一步提升重建效果。

第四，轻量化与高效推理将成为临床应用的关键。随着医学影像超分辨率方法向临床应用的推进，如何在保持高生成质量的同时实现轻量化和高效推理，将成为研究的重要目标。这包括模型压缩、知识蒸馏、快速采样等技术的研究与应用。

第五，针对医学影像特殊性的定制化方法将得到发展。医学影像具有其独特特性，未来研究将更加注重针对医学影像的特殊需求设计定制化的方法，例如考虑医学影像的重复模式、结构特性等，以进一步提升重建效果。

第六，可解释性与可信度将受到更多关注。在医学应用中，模型的可解释性和可信度至关重要。未来研究将更加注重提升模型的可解释性，使临床医生能够理解模型的决策过程，从而增强对模型结果的信任。

第七，大规模预训练模型的应用将成为趋势。随着大规模视觉预训练模型的发展，如何将这些模型应用于医学影像超分辨率任务，通过迁移学习提升性能，将是未来研究的重要方向。

### 3.3 研究现状分析结论

通过对医学影像超分辨率研究现状的全面分析，我们得出以下结论：

首先，非生成式模型虽然在速度和稳定性方面具有优势，但在细节丰富度上存在明显局限性，难以满足高质量医学影像重建的需求。特别是在处理严重退化的低分辨率图像时，这些方法往往无法恢复已经丢失的细节信息。

其次，生成式模型虽然在生成质量方面具有显著优势，但各自存在明显的局限性。扩散模型的计算成本高、推理速度慢；GAN的训练不稳定性和潜在的伪影问题；传统自回归模型对离散量化的依赖限制了表示能力。这些问题严重限制了生成式模型在医学影像超分辨率中的广泛应用。

第三，现有方法在计算效率与生成质量的平衡方面存在不足，无法同时满足高质量和高效推理的需求。这限制了这些方法在临床实际应用中的推广。

第四，针对医学影像特殊性的定制化方法仍有发展空间。现有方法往往直接借鉴自然图像超分辨率的方法，没有充分考虑医学影像的特殊性，如重复模式、结构特性等。

基于以上分析，我们认为未来的研究应该重点关注以下几个方面：一是开发融合生成式模型与非生成式模型优势的混合方法，在保持高生成质量的同时提升计算效率；二是研究非量化的连续空间建模方法，突破离散量化对模型表示能力的限制；三是针对医学影像的特殊性设计定制化的方法，充分考虑医学影像的独特特性；四是开发轻量化和高效推理的方法，推动医学影像超分辨率技术向临床应用转化。

具体而言，我们提出的基于Diffusion Loss与非量化VAR（VARDiff）结构的自回归超分模型，正是针对上述问题的一种解决方案。该方法在保留VAR模型高效逐层生成特性的同时，引入Diffusion Loss对非量化的视觉token进行细粒度约束，使模型在重建过程中能够更好地保留纹理细节与结构信息。相比对整幅图像进行扩散建模的传统方法，我们的设计仅在token层面引入轻量化的扩散正则，从而在几乎不增加计算开销的前提下，有效提升了生成结果的真实性与稳定性，实现了兼顾效率与高保真的医学影像超分重建。

在传统的VAR框架中，图像通常经过VAR Tokenizer量化后，再由自回归模型逐步预测下一个token。然而，由于自回归模型的训练机制依赖于交叉熵损失（Cross-Entropy Loss），这就要求输入必须是离散化的量化token，从而限制了模型的表示能力与生成质量。为突破这一限制，我们在自回归模型中引入了Diffusion模块与Diffusion Loss，使得VAR不再依赖量化过程。通过在连续空间中进行建模与优化，模型能够更充分地保留图像的细节与结构信息，从而显著提升生成质量与视觉真实感。

## 参考文献

[1] 来源：综述2中的Ref/25-XXXX.pdf，参考文献[1]

[2] 来源：综述2中的Ref/25-XXXX.pdf，参考文献[2]

[3] Chang, H., et al. MaskGIT: Masked Generative Image Transformer. CVPR 2022. 来源：综述1中的Ref/related work/3-Chang_MaskGIT_Masked_Generative_Image_Transformer_CVPR_2022_paper.pdf

[4] 来源：综述2中的Ref/25-XXXX.pdf，参考文献[4]

[5] Chung, H., et al. Come-Closer-Diffuse-Faster: Accelerating Conditional Diffusion Models for Inverse Problems Through Stochastic Contraction. CVPR 2022. 来源：综述1中的Ref/related work/5-Chung_Come-Closer-Diffuse-Faster_Accelerating_Conditional_Diffusion_Models_for_Inverse_Problems_Through_Stochastic_CVPR_2022_paper.pdf

[6] Chung, H., et al. Score-based diffusion models for accelerated MRI. 来源：综述1中的Ref/related work/6-Score-based diffusion.pdf

[7] 来源：综述2中的Ref/25-XXXX.pdf，参考文献[7]

[8] Feng, C., et al. Multi-stage feature fusion mechanism for multi-contrast SR. 来源：综述2中的Ref/25-XXXX.pdf，参考文献[8]

[9] 来源：综述3中的Ref/58-XXXX.pdf，参考文献[9]

[10] Feng, C., et al. Multi-modal Transformer for accelerated MR Imaging. 来源：综述1中的Ref/introduction/12-multi-modal transformer for accelerated MR Imaging.pdf

[11] Feng, C., et al. Multi-contrast MRI. 来源：综述1中的Ref/related work/11-multi-contrast mri.pdf

[12] 来源：综述2中的Ref/25-XXXX.pdf，参考文献[12]

[13] Gao, J., et al. Implicit Diffusion Models for Continuous Super-Resolution. CVPR 2023. 来源：综述1中的Ref/related work/13-Gao_Implicit_Diffusion_Models_for_Continuous_Super-Resolution_CVPR_2023_paper.pdf

[14] 来源：综述2中的Ref/25-XXXX.pdf，参考文献[14]

[15] Georgescu, M., et al. Multimodal Multi-Head Convolutional Attention With Various Kernel Sizes for Medical. WACV 2023. 来源：综述1中的Ref/related work/15-Georgescu_Multimodal_Multi-Head_Convolutional_Attention_With_Various_Kernel_Sizes_for_Medical_WACV_2023_paper.pdf

[16] Asilomar09_CompressiveSuperRes. 来源：综述2中的Ref/16-Asilomar09_CompressiveSuperRes.pdf

[17] Ho, J., et al. Denoising Diffusion Probabilistic Models. NeurIPS 2020. 来源：综述1中的Ref/related work/17-NeurIPS-2020-denoising-diffusion-probabilistic-models-Paper.pdf

[18] 来源：综述3中的Ref/58-XXXX.pdf，参考文献[18]

[19] 来源：综述2中的Ref/25-XXXX.pdf，参考文献[19]

[20] 来源：综述2中的Ref/25-XXXX.pdf，参考文献[20]

[21] 来源：综述2中的Ref/25-XXXX.pdf，参考文献[21]

[22] 来源：综述2中的Ref/25-XXXX.pdf，参考文献[22]

[23] Lee, D., et al. Autoregressive Image Generation Using Residual Quantization. CVPR 2022. 来源：综述1中的Ref/related work/23-Lee_Autoregressive_Image_Generation_Using_Residual_Quantization_CVPR_2022_paper.pdf

[24] Lei, Y., et al. Decomposition-Based Variational Network for Multi-Contrast MRI Super-Resolution and Reconstruction. ICCV 2023. 来源：综述1中的Ref/related work/24-Lei_Decomposition-Based_Variational_Network_for_Multi-Contrast_MRI_Super-Resolution_and_Reconstruction_ICCV_2023_paper.pdf

[25] Li, Q., et al. Transformer-Empowered Multi-Scale Contextual Matching and Aggregation for Multi-Contrast MRI Super-Resolution. CVPR 2022. 来源：综述1中的Ref/related work/25-Li_Transformer-Empowered_Multi-Scale_Contextual_Matching_and_Aggregation_for_Multi-Contrast_MRI_Super-Resolution_CVPR_2022_paper.pdf

[26] 来源：综述3中的Ref/58-XXXX.pdf，参考文献[26]

[27] Li, Y., et al. Rethinking Diffusion Model for Multi-Contrast MRI Super-Resolution. CVPR 2024. 来源：综述1中的Ref/related work/27-Li_Rethinking_Diffusion_Model_for_Multi-Contrast_MRI_Super-Resolution_CVPR_2024_paper.pdf

[28] 来源：综述2中的Ref/25-XXXX.pdf，参考文献[28]

[29] Li, Y., et al. SRDiff: Single Image Super-Resolution with Diffusion Probabilistic Models. 来源：综述1中的Ref/related work/29-SRDiff.pdf

[30] 来源：综述2中的Ref/25-XXXX.pdf，参考文献[30]

[31] 来源：综述3中的Ref/58-XXXX.pdf，参考文献[31]

[32] Exploring controllable autoregressive generation. 来源：综述1中的Ref/related work/32-Exploring controllabe.pdf

[33] Liang, J., et al. SwinIR: Image Restoration Using Swin Transformer. ICCVW 2021. 来源：综述1中的Ref/related work/33-Liang_SwinIR_Image_Restoration_Using_Swin_Transformer_ICCVW_2021_paper.pdf

[34] 来源：综述2中的Ref/25-XXXX.pdf，参考文献[34]

[35] 来源：综述3中的Ref/58-XXXX.pdf，参考文献[35]

[36] 来源：综述3中的Ref/58-XXXX.pdf，参考文献[36]

[37] 来源：综述2中的Ref/25-XXXX.pdf，参考文献[37]

[38] Mao, Y., et al. Disc-Diff: A conditional diffusion model for multi-contrast MRI. 来源：综述1中的Ref/related work/38-Disc-Diff.pdf

[39] 来源：综述2中的Ref/25-XXXX.pdf，参考文献[39]

[40] 来源：综述2中的Ref/25-XXXX.pdf，参考文献[40]

[41] Super resolution methods in MRI: Can they improve the trade-off between resolution, signal-to-noise ratio, and acquisition time? 来源：综述1中的Ref/introduction/41-Super_resolution_methods_in_MRI_Can_they.pdf

[42] Visual autoregressive modeling: Scalable image generation via next-scale prediction. NeurIPS 2024. 来源：综述1中的Ref/related work/42-NeurIPS-2024-visual-autoregressive-modeling-scalable-image-generation-via-next-scale-prediction-Paper-Conference.pdf

[43] 来源：综述3中的Ref/58-XXXX.pdf，参考文献[43]

[44] Rombach, R., et al. High-Resolution Image Synthesis With Latent Diffusion Models. CVPR 2022. 来源：综述1中的Ref/related work/44-Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.pdf

[45] Autoregressive Model Beats Diffusion: Transformers are Efficient and Flexible Tool for Image Generation. 来源：综述1中的Ref/related work/45-Autoregressive Model Beats Diffusion.pdf

[46] Visual autoregressive modeling: Scalable image generation via next-scale prediction. NeurIPS 2024. 来源：综述1中的Ref/related work/46-NeurIPS-2024-visual-autoregressive-modeling-scalable-image-generation-via-next-scale-prediction-Paper-Conference.pdf

[47] van den Oord, A., et al. Neural Discrete Representation Learning. NIPS 2017. 来源：综述1中的Ref/related work/47-NIPS-2017-neural-discrete-representation-learning-Paper.pdf

[48] Varformer: Vision Transformer with Variable Resolution. 来源：综述1中的Ref/related work/48-Varformer.pdf

[49] Emu3: Next-token prediction. 来源：综述1中的Ref/related work/49-Emu3-Next-token prediction.pdf

[50] 来源：综述3中的Ref/58-XXXX.pdf，参考文献[49]

[51] Quantifying structural. 来源：综述1中的Ref/introduction/51-Quantitying structural.pdf

[52] 来源：综述2中的Ref/25-XXXX.pdf，参考文献[52]

[53] 来源：综述2中的Ref/25-XXXX.pdf，参考文献[53]

[54] Controllable autoregressive modeling. 来源：综述1中的Ref/related work/54-Controllable autogressive modeling.pdf

[55] 来源：综述2中的Ref/25-XXXX.pdf，参考文献[55]

[56] Var-Clip. 来源：综述1中的Ref/related work/56-Var-Clip.pdf

[57] 来源：综述2中的Ref/25-XXXX.pdf，参考文献[57]

[58] Zhang, Y., et al. MR Image Super-Resolution With Squeeze and Excitation Reasoning Attention Network. CVPR 2021. 来源：综述1中的Ref/related work/58-Zhang_MR_Image_Super-Resolution_With_Squeeze_and_Excitation_Reasoning_Attention_Network_CVPR_2021_paper.pdf

[59] Guo, S., et al. LAR-SR: A Local Autoregressive Model for Image Super-Resolution. CVPR 2022. 来源：综述1中的Ref/related work/16-Guo_LAR-SR_A_Local_Autoregressive_Model_for_Image_Super-Resolution_CVPR_2022_paper.pdf

[60] Autoregressive Image Generation without Vector Quantization. 来源：mar.pdf

[61] GAN based super-resolution methods. 来源：GAN based super-resolution.pdf（具体参考文献见该PDF文件中的相关引用）

[62] Super-resolution CT image reconstruction. 来源：综述1中的Ref/introduction/19-Super-resolution ct image reconstruction.pdf

[63] Low cost implementation. 来源：综述1中的Ref/related work/21-low_Cost Imolementation.pdf

[64] Yang, J., et al. Image super-resolution via sparse representation. 来源：综述2中的Ref/25-yang_jianchao.pdf

[65] Low cost implementation. 来源：综述2中的Ref/6-low_Cost Imolementation.pdf


