我目前的idea如下：传统的插值、稀疏表示、字典学习等方法由于无法准确建模低分辨率与高分辨率影像间的复杂非线性关系，难以恢复真实的高频细节。随着深度学习的发展，卷积神经网络（CNN）和Transformer等模型例如VAR在图像重建任务中显著提升了性能，但仍容易出现过度平滑、纹理丢失等问题。近年兴起的扩散模型（Diffusion Model）能更稳定地建模复杂分布，在医学影像生成中表现优异，但其多步迭代推理导致计算代价高、速度慢，限制了临床应用。
为此，我们提出了一种基于 Diffusion Loss 与非量化 VAR（VARDiff）结构 的自回归超分模型。该方法在保留 VAR 模型高效逐层生成特性的同时，引入 Diffusion Loss 对非量化的视觉 token 进行细粒度约束，使模型在重建过程中能够更好地保留纹理细节与结构信息。相比对整幅图像进行扩散建模的传统方法，我们的设计仅在 token 层面引入轻量化的扩散正则，从而在几乎不增加计算开销的前提下，有效提升了生成结果的真实性与稳定性，实现了兼顾效率与高保真的医学影像超分重建。
在传统的 VAR 框架中，图像通常经过 VAR Tokenizer 量化后，再由自回归模型逐步预测下一个 token。然而，由于自回归模型的训练机制依赖于 交叉熵损失（Cross-Entropy Loss），这就要求输入必须是离散化的量化 token，从而限制了模型的表示能力与生成质量。为突破这一限制，我们在自回归模型中引入了 Diffusion 模块与 Diffusion Loss，使得 VAR 不再依赖量化过程。通过在连续空间中进行建模与优化，模型能够更充分地保留图像的细节与结构信息，从而显著提升生成质量与视觉真实感。
￼